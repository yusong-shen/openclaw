import AVFoundation
import Foundation
import OSLog
import Speech

/// Background listener that keeps the voice-wake pipeline alive outside the settings test view.
actor VoiceWakeRuntime {
    static let shared = VoiceWakeRuntime()

    private let logger = Logger(subsystem: "com.steipete.clawdis", category: "voicewake.runtime")

    private var recognizer: SFSpeechRecognizer?
    private var audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var lastHeard: Date?
    private var captureStartedAt: Date?
    private var captureTask: Task<Void, Never>?
    private var capturedTranscript: String = ""
    private var isCapturing: Bool = false
    private var heardBeyondTrigger: Bool = false
    private var cooldownUntil: Date?
    private var currentConfig: RuntimeConfig?

    // Tunables
    private let silenceWindow: TimeInterval = 1.0
    private let captureHardStop: TimeInterval = 8.0
    private let debounceAfterSend: TimeInterval = 0.35

    struct RuntimeConfig: Equatable {
        let triggers: [String]
        let micID: String?
        let localeID: String?
    }

    func refresh(state: AppState) async {
        let snapshot = await MainActor.run { () -> (Bool, RuntimeConfig) in
            let enabled = state.swabbleEnabled
            let config = RuntimeConfig(
                triggers: state.swabbleTriggerWords,
                micID: state.voiceWakeMicID.isEmpty ? nil : state.voiceWakeMicID,
                localeID: state.voiceWakeLocaleID.isEmpty ? nil : state.voiceWakeLocaleID)
            return (enabled, config)
        }

        guard voiceWakeSupported, snapshot.0 else {
            self.stop()
            return
        }

        guard PermissionManager.voiceWakePermissionsGranted() else {
            self.logger.debug("voicewake runtime not starting: permissions missing")
            self.stop()
            return
        }

        let config = snapshot.1

        if config == self.currentConfig, self.recognitionTask != nil {
            return
        }

        self.stop()
        await self.start(with: config)
    }

    private func start(with config: RuntimeConfig) async {
        do {
            self.configureSession(localeID: config.localeID)

            guard let recognizer, recognizer.isAvailable else {
                self.logger.error("voicewake runtime: speech recognizer unavailable")
                return
            }

            self.recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            self.recognitionRequest?.shouldReportPartialResults = true
            guard let request = self.recognitionRequest else { return }

            let input = self.audioEngine.inputNode
            let format = input.outputFormat(forBus: 0)
            input.removeTap(onBus: 0)
            input.installTap(onBus: 0, bufferSize: 2048, format: format) { [weak request] buffer, _ in
                request?.append(buffer)
            }

            self.audioEngine.prepare()
            try self.audioEngine.start()

            self.currentConfig = config
            self.lastHeard = Date()
            self.cooldownUntil = nil

            self.recognitionTask = recognizer.recognitionTask(with: request) { [weak self] result, error in
                guard let self else { return }
                let transcript = result?.bestTranscription.formattedString
                Task { await self.handleRecognition(transcript: transcript, error: error, config: config) }
            }

            self.logger.info("voicewake runtime started")
        } catch {
            self.logger.error("voicewake runtime failed to start: \(error.localizedDescription, privacy: .public)")
            self.stop()
        }
    }

    private func stop() {
        self.captureTask?.cancel()
        self.captureTask = nil
        self.isCapturing = false
        self.capturedTranscript = ""
        self.captureStartedAt = nil
        self.recognitionTask?.cancel()
        self.recognitionTask = nil
        self.recognitionRequest?.endAudio()
        self.recognitionRequest = nil
        self.audioEngine.inputNode.removeTap(onBus: 0)
        self.audioEngine.stop()
        self.currentConfig = nil
        self.logger.debug("voicewake runtime stopped")

        Task { @MainActor in
            VoiceWakeOverlayController.shared.dismiss()
        }
    }

    private func configureSession(localeID: String?) {
        let locale = localeID.flatMap { Locale(identifier: $0) } ?? Locale(identifier: Locale.current.identifier)
        self.recognizer = SFSpeechRecognizer(locale: locale)
    }

    private func handleRecognition(
        transcript: String?,
        error: Error?,
        config: RuntimeConfig) async
    {
        if let error {
            self.logger.debug("voicewake recognition error: \(error.localizedDescription, privacy: .public)")
        }

        guard let transcript else { return }

        let now = Date()
        if !transcript.isEmpty {
            self.lastHeard = now
            if self.isCapturing {
                self.capturedTranscript = Self.trimmedAfterTrigger(transcript, triggers: config.triggers)
                self.updateHeardBeyondTrigger(with: transcript)
                let snapshot = self.capturedTranscript
                await MainActor.run {
                    VoiceWakeOverlayController.shared.showPartial(transcript: snapshot)
                }
            }
        }

        if self.isCapturing { return }

        if Self.matches(text: transcript, triggers: config.triggers) {
            if let cooldown = cooldownUntil, now < cooldown {
                return
            }
            await self.beginCapture(transcript: transcript, config: config)
        }
    }

    private static func matches(text: String, triggers: [String]) -> Bool {
        guard !text.isEmpty else { return false }
        let normalized = text.lowercased()
        for trigger in triggers {
            let t = trigger.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
            if t.isEmpty { continue }
            if normalized.contains(t) { return true }
        }
        return false
    }

    private func beginCapture(transcript: String, config: RuntimeConfig) async {
        self.isCapturing = true
        self.capturedTranscript = Self.trimmedAfterTrigger(transcript, triggers: config.triggers)
        self.captureStartedAt = Date()
        self.cooldownUntil = nil
        self.heardBeyondTrigger = self.textHasBeyondTriggerContent(transcript)

        let snapshot = self.capturedTranscript
        await MainActor.run {
            VoiceWakeOverlayController.shared.showPartial(transcript: snapshot)
        }

        await MainActor.run { AppStateStore.shared.triggerVoiceEars(ttl: nil) }

        self.captureTask?.cancel()
        self.captureTask = Task { [weak self] in
            guard let self else { return }
            await self.monitorCapture(config: config)
        }
    }

    private func monitorCapture(config: RuntimeConfig) async {
        let start = self.captureStartedAt ?? Date()
        let hardStop = start.addingTimeInterval(self.captureHardStop)
        var silentStrikes = 0

        while self.isCapturing {
            let now = Date()
            if now >= hardStop {
                await self.finalizeCapture(config: config)
                return
            }

            if let last = self.lastHeard, now.timeIntervalSince(last) >= self.silenceWindow {
                silentStrikes += 1
                if silentStrikes >= 2 {
                    await self.finalizeCapture(config: config)
                    return
                }
            } else {
                silentStrikes = 0
            }

            try? await Task.sleep(nanoseconds: 200_000_000)
        }
    }

    private func finalizeCapture(config: RuntimeConfig) async {
        guard self.isCapturing else { return }
        self.isCapturing = false
        self.captureTask?.cancel()
        self.captureTask = nil

        let finalTranscript = self.capturedTranscript.trimmingCharacters(in: .whitespacesAndNewlines)
        self.capturedTranscript = ""
        self.captureStartedAt = nil
        self.lastHeard = nil
        let heardBeyondTrigger = self.heardBeyondTrigger
        self.heardBeyondTrigger = false

        await MainActor.run { AppStateStore.shared.stopVoiceEars() }

        guard !finalTranscript.isEmpty else {
            await MainActor.run { VoiceWakeOverlayController.shared.dismiss(reason: .empty) }
            self.cooldownUntil = Date().addingTimeInterval(self.debounceAfterSend)
            self.restartRecognizer()
            return
        }

        let forwardConfig = await MainActor.run { AppStateStore.shared.voiceWakeForwardConfig }
        let delay: TimeInterval = heardBeyondTrigger ? 1.0 : 3.0
        await MainActor.run {
            VoiceWakeOverlayController.shared.presentFinal(
                transcript: finalTranscript,
                forwardConfig: forwardConfig,
                delay: delay)
        }

        self.cooldownUntil = Date().addingTimeInterval(self.debounceAfterSend)
        self.restartRecognizer()
    }

    private func restartRecognizer() {
        // Restart the recognizer so we listen for the next trigger with a clean buffer.
        let current = self.currentConfig
        self.stop()
        if let current {
            Task { await self.start(with: current) }
        }
    }

    private func textHasBeyondTriggerContent(_ text: String) -> Bool {
        let words = text.split(whereSeparator: { $0.isWhitespace })
        return words.count > 1
    }

    private func updateHeardBeyondTrigger(with transcript: String) {
        if !self.heardBeyondTrigger, self.textHasBeyondTriggerContent(transcript) {
            self.heardBeyondTrigger = true
        }
    }

    private static func trimmedAfterTrigger(_ text: String, triggers: [String]) -> String {
        let lower = text.lowercased()
        for trigger in triggers {
            let token = trigger.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
            guard !token.isEmpty, let range = lower.range(of: token) else { continue }
            let after = range.upperBound
            let trimmed = text[after...].trimmingCharacters(in: .whitespacesAndNewlines)
            return String(trimmed)
        }
        return text
    }

    #if DEBUG
    static func _testMatches(text: String, triggers: [String]) -> Bool {
        self.matches(text: text, triggers: triggers)
    }
    #endif
}
